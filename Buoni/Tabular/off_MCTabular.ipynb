{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-23T16:23:57.198406Z",
     "start_time": "2024-08-23T16:23:56.280460Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from abc import ABC\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:23:57.222720Z",
     "start_time": "2024-08-23T16:23:57.202323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_policy(self):\n",
    "        for state in range(self.state_size):\n",
    "            best_action = np.argmax(self.table[state])\n",
    "            for action in range(self.action_size):\n",
    "                if action == best_action:\n",
    "                    self.policy[state][action] = 1 - self.epsilon + (self.epsilon / self.action_size)\n",
    "                else:\n",
    "                    self.policy[state][action] = self.epsilon / self.action_size\n",
    "\n",
    "    def update_table(self, episode):\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            \n",
    "            if self.visit_type == 'first':\n",
    "                if (state, action) not in visited:\n",
    "                    visited.add((state, action))\n",
    "                    self.returns[state][action].append(G)\n",
    "                    self.table[state][action] = np.mean(self.returns[state][action])\n",
    "            \n",
    "            elif self.visit_type == 'every':\n",
    "                self.returns[state][action].append(G)\n",
    "                self.table[state][action] = np.mean(self.returns[state][action])\n"
   ],
   "id": "10457c204ee23244",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def update_policy(self, episode):\\n        for state in range(self.state_size):\\n            best_action = np.argmax(self.table[state])\\n            for action in range(self.action_size):\\n                if action == best_action:\\n                    self.policy[state][action] = 1 - self.epsilon + (self.epsilon / self.action_size)\\n                else:\\n                    self.policy[state][action] = self.epsilon / self.action_size\\n        #QUESTO VIENE FATTO PER OGNI EPISODIO\\n        #UPDATE RULE:\\n        # Q(s_t, a_t) = Q(s_t,a_t) + v(G_t^pi - Q(s_t,a_t))\\n        # con v = (W)/(C(s_t,a_t))\\n        #Per ogni coppia stato-azione di un epsidio aggiorniamo G\\n        # Viene calcolato al contrario partendo dalla fine andando all'inizio\\n        G = 0.0\\n        #IMPORTANCE SAMPLING:\\n        #Andiamo a pesare la differenza tra policy esplorativa e policy target\\n        W = 1.0\\n        for t in reversed(range(len(episode))):\\n            state, action, reward = episode[t]\\n            if isinstance(state, tuple):\\n                state = state[0]\\n            G = self.gamma * G + reward\\n            self.C[state][action] += W\\n            self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\\n            if action != np.argmax(self.target_policy(state)):\\n                break\\n            W /= self.behavior_policy(state)[action]\\n\\n    def update_table(self, episode):\\n        G = 0\\n        visited = set()\\n        for t in reversed(range(len(episode))):\\n            state, action, reward = episode[t]\\n            G = self.gamma * G + reward\\n            \\n            if self.visit_type == 'first':\\n                if (state, action) not in visited:\\n                    visited.add((state, action))\\n                    self.returns[state][action].append(G)\\n                    self.table[state][action] = np.mean(self.returns[state][action])\\n            \\n            elif self.visit_type == 'every':\\n                self.returns[state][action].append(G)\\n                self.table[state][action] = np.mean(self.returns[state][action])\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:24:02.767472Z",
     "start_time": "2024-08-23T16:24:02.756111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_policy(nA):\n",
    "    return lambda observation: np.ones(nA, dtype=float) / nA\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon=0.1):\n",
    "    def policy_fn(state):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        nA = len(Q[state])\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ],
   "id": "13fe7915d1c365ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class tabular_agent(ABC):\n",
    "\n",
    "    def __init__(self, env, epsilon_start, epsilon_decay, epsilon_min, episodes, gamma, visit_type):\n",
    "        self.env_name = env\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.visit_type = visit_type\n",
    "        self.action_space = self.env.action_space\n",
    "        self.action_size = self.env.action_space.n\n",
    "        # For FrozenLake-v1, state space is discrete, so no binning needed\n",
    "        self.state_size = self.env.observation_space.n\n",
    "        # Definition of the Q-table\n",
    "        self.table = np.zeros([self.state_size, self.action_size])\n",
    "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        #Contatore coppie stato-azione per mediare i ritorni incrementali\n",
    "        self.C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.returns = {state: {action: [] for action in range(self.action_size)} for state in range(self.state_size)}\n",
    "        #Policy esplorativa\n",
    "        self.behavior_policy = random_policy(env.action_space.n)\n",
    "        #Policy da ottimizzare ed usare\n",
    "        self.target_policy = epsilon_greedy_policy(self.Q, epsilon=self.epsilon)\n",
    "        self.score = []\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def generate_episode(self):\n",
    "        #3) Generazione episodio singolo\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            #action = self.select_action(state)\n",
    "            action_probs = self.behavior_policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def update(self, episode, episode_num):\n",
    "        #QUESTO VIENE FATTO PER OGNI EPISODIO\n",
    "        print(f\"UPDATE EPISODE: {episode_num}\\n\")\n",
    "        #UPDATE RULE:\n",
    "        # Q(s_t, a_t) = Q(s_t,a_t) + v(G_t^pi - Q(s_t,a_t))\n",
    "        # con v = (W)/(C(s_t,a_t))\n",
    "\n",
    "        #Per ogni coppia stato-azione di un epsidio aggiorniamo G\n",
    "        # Viene calcolato al contrario partendo dalla fine andando all'inizio\n",
    "        G = 0.0\n",
    "        visited = set()\n",
    "        #IMPORTANCE SAMPLING:\n",
    "        #Andiamo a pesare la differenza tra policy esplorativa e policy target\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            self.C[state][action] += W\n",
    "            self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
    "            if action != np.argmax(self.target_policy(state)):\n",
    "                break\n",
    "            W /= self.behavior_policy(state)[action]\n",
    "    \n",
    "    def update_table(self, episode):\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            \n",
    "            if self.visit_type == 'first':\n",
    "                if (state, action) not in visited:\n",
    "                    visited.add((state, action))\n",
    "                    self.returns[state][action].append(G)\n",
    "                    self.table[state][action] = np.mean(self.returns[state][action])\n",
    "            \n",
    "            elif self.visit_type == 'every':\n",
    "                self.returns[state][action].append(G)\n",
    "                self.table[state][action] = np.mean(self.returns[state][action])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        return np.random.choice(np.arange(self.action_size), p=self.behavior_policy[state])\n",
    "\n",
    "    def learn(self):\n",
    "        #2) Learn, chiamato giu\n",
    "        print(f\"\\nINIZIO LEARNING\\n\")\n",
    "        for e in tqdm(range(self.episodes), desc=\"Learning\"):\n",
    "            episode = self.generate_episode()\n",
    "            \"\"\"self.update_table(episode)\n",
    "            self.update_policy()\"\"\"\n",
    "            self.update(episode,2)\n",
    "            self.update_epsilon()\n",
    "            self.score.append(sum([x[2] for x in episode]))  # Sum of rewards for this episode\n",
    "\n",
    "    def plot_learning(self, N, title=None, filename=\"\"):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Convert scores to numpy array\n",
    "        scores = np.array(self.score)\n",
    "        \n",
    "        # Compute the moving average\n",
    "        mean_score = np.convolve(scores, np.ones(N)/N, mode='valid')\n",
    "        \n",
    "        # Compute the moving variance\n",
    "        variance = np.array([np.var(scores[max(0, i-N+1):i+1]) for i in range(len(scores))])\n",
    "        variance = np.convolve(variance, np.ones(N)/N, mode='valid')\n",
    "        \n",
    "        # Adjust x values for the moving average plot\n",
    "        x_values = np.arange(len(mean_score)) + (N // 2)\n",
    "\n",
    "        # Plot the original scores\n",
    "        plt.plot(scores, label='Score')\n",
    "        \n",
    "        # Plot the moving average\n",
    "        plt.plot(x_values, mean_score, label='Moving Average', color='orange')\n",
    "        \n",
    "        # Plot the moving variance\n",
    "        plt.plot(x_values, variance, label='Moving Variance', color='red')\n",
    "        \n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.savefig(self.env_name + filename)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        np.save(path, self.table)\n",
    "        print(f\"Q-table saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        if os.path.exists(path):\n",
    "            self.table = np.load(path)\n",
    "            print(f\"Q-table loaded from {path}\")\n",
    "        else:\n",
    "            print(f\"File {path} does not exist.\")\n",
    "\n",
    "    def simulate(self):\n",
    "        env = gym.make(self.env_name, render_mode=\"human\")\n",
    "        self.epsilon = -1  # Set epsilon to -1 to disable exploration\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        env.close()\n"
   ],
   "id": "a7310dc91d2aeb7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    env = 'FrozenLake-v1'  # Environment set for FrozenLake\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_decay = 0.995  # Slower decay for more episodes\n",
    "    epsilon_min = 0.01\n",
    "    episodes = 30000  # Adjust as needed\n",
    "    gamma = 0.99\n",
    "    \n",
    "    # Initialize the Monte Carlo agent with either 'first' or 'every' visit type\n",
    "    visit_type = 'first'  # Change to 'every' for Every-Visit Monte Carlo\n",
    "    #1) Creazione agente\n",
    "    agent = tabular_agent(env, epsilon_start, epsilon_decay, epsilon_min, episodes, gamma, visit_type)\n",
    "    #2) Inizio Learn\n",
    "    agent.learn()\n",
    "    \n",
    "    \n",
    "    #n) Parte sui grafici\n",
    "    agent.plot_learning(100, filename=f\"_MonteCarlo_{visit_type}_visit\")\n",
    "    \n",
    "    #m) Salvo modello\n",
    "    agent.save_model(f\"models/frozenlake_monte_carlo_{visit_type}_visit.npy\")"
   ],
   "id": "2b1f40c7560f5d15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#o) Fatto modello, fai simulazione con FV\n",
    "    agent.simulate()"
   ],
   "id": "8acd165ce7a4f289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#p) Stessa cosa ma con FV\n",
    "    visit_type = 'every'  # Change to 'every' for Every-Visit Monte Carlo\n",
    "    agent = tabular_agent(env, epsilon_start, epsilon_decay, epsilon_min, episodes, gamma, visit_type)\n",
    "\n",
    "    agent.learn()\n",
    "    agent.plot_learning(100, filename=f\"_MonteCarlo_{visit_type}_visit\")\n",
    "\n",
    "    # Save the trained model\n",
    "    agent.save_model(f\"models/frozenlake_monte_carlo_{visit_type}_visit.npy\")\n",
    "\n",
    "    agent.simulate()"
   ],
   "id": "158e9fd6befad789"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

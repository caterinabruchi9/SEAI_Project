{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "def create_env():\n",
    "    return gym.make('FrozenLake-v1', is_slippery=False)  # or True, depending on your use case\n",
    "\n",
    "# Initialize Q-table\n",
    "def initialize_q_table(env):\n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    return np.zeros((number_of_states, number_of_actions))\n",
    "\n",
    "# Decay function for epsilon\n",
    "def decay_function(episode, total_train_episodes, min_epsilon=0.01):\n",
    "    return max(min_epsilon, min(1.0, 1.0 - np.log10((episode + 1) / (total_train_episodes * 0.1))))\n",
    "\n",
    "# Choose action based on epsilon-greedy policy\n",
    "def choose_action(q_table, state, epsilon, env):\n",
    "    if np.random.random() <= epsilon:\n",
    "        return env.action_space.sample()  # Exploration\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploitation\n",
    "\n",
    "# Generate an episode\n",
    "def Generate_episode(epsilon, q_table, env, max_env_steps):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    \n",
    "    for step in range(max_env_steps):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        action = choose_action(q_table, state, epsilon, env)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        trajectory.append([state, action, reward])\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory, total_reward\n",
    "\n",
    "# Monte Carlo algorithm\n",
    "def Monte_Carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon):\n",
    "    q_table = initialize_q_table(env)\n",
    "    visits_counter = np.zeros_like(q_table)\n",
    "    rewards = []\n",
    "    \n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(total_train_episodes):\n",
    "        epsilon = decay_function(episode, total_train_episodes, min_epsilon)\n",
    "        trajectory, total_reward = Generate_episode(epsilon, q_table, env, max_env_steps)\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(trajectory))):\n",
    "            state, action, reward = trajectory[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # Convert state to integer if it's a dictionary\n",
    "            if isinstance(state, dict):\n",
    "                state = state.get('state_key', 0)  # Adjust 'state_key' to the appropriate key in your dictionary\n",
    "            \n",
    "            if not [state, action] in [[x[0], x[1]] for x in trajectory[0:t]]:\n",
    "                \n",
    "                if isinstance(state, tuple):\n",
    "                    state = state[0]  # Adjust for your state representation\n",
    "                                    \n",
    "                visits_counter[state, action] += 1\n",
    "                q_table[state, action] += (G - q_table[state, action]) / visits_counter[state, action]\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            rewards.append(total_reward)\n",
    "            print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    return q_table, rewards\n",
    "\n",
    "# Plotting the training rewards\n",
    "def plot_rewards(rewards):\n",
    "    x = np.linspace(0, len(rewards) * 50, len(rewards))\n",
    "    plt.plot(x, rewards, label='Monte Carlo')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Testing the policy\n",
    "def test_policy(env, q_table, num_episodes=10):\n",
    "    rewards = []\n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_env_steps):\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            env.render()\n",
    "            if done:\n",
    "                env.render()\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_rewards)\n",
    "        print(f\"Episode {episode}, reward {total_rewards}\")\n",
    "    \n",
    "    print(f\"Average score: {np.mean(rewards)}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    n_times = 5\n",
    "    total_train_episodes = 10000\n",
    "    gamma = 0.99\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    env = create_env()\n",
    "\n",
    "    MC_tables = []\n",
    "    MC_rewards = []\n",
    "\n",
    "    for number in range(n_times):\n",
    "        print(f\"\\n ********** Training number {number}\")\n",
    "        q_table, rewards = Monte_Carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        MC_tables.append(q_table)\n",
    "        MC_rewards.append(rewards)\n",
    "\n",
    "    MC_mean = np.mean(np.array(MC_rewards), axis=0)\n",
    "    np.save('MC_mean.npy', MC_mean)\n",
    "    np.save('MC_tables.npy', MC_tables)\n",
    "\n",
    "    plot_rewards(MC_mean)\n",
    "\n",
    "    # Load Q-tables and test\n",
    "    q_tables = np.load('MC_tables.npy', allow_pickle=True)\n",
    "    q_table = q_tables[0]\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "    test_policy(env, q_table, num_episodes=5)\n",
    "    \n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

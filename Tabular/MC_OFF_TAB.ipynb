{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:54:49.360453Z",
     "start_time": "2024-08-28T09:54:49.030703Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "def create_env():\n",
    "    return gym.make('FrozenLake-v1', is_slippery=False)  # or True, depending on your use case\n",
    "\n",
    "# Initialize Q-table\n",
    "def initialize_q_table(env):\n",
    "    number_of_states = env.observation_space.n #16, campo 4x4\n",
    "    number_of_actions = env.action_space.n #4, su,giu,dx,sx\n",
    "    return np.zeros((number_of_states, number_of_actions))\n",
    "\n",
    "# Decay function for epsilon\n",
    "def decay_function(episode, total_train_episodes, min_epsilon=0.01):\n",
    "    return max(min_epsilon, min(1.0, 1.0 - np.log10((episode + 1) / (total_train_episodes * 0.1))))\n",
    "\n",
    "# Choose action based on epsilon-greedy policy (Behavior Policy)\n",
    "def choose_action(q_table, state, epsilon, env):\n",
    "    if np.random.random() <= epsilon:\n",
    "        return env.action_space.sample()  # Exploration\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploitation\n",
    "\n",
    "# Generate an episode\n",
    "def generate_episode(epsilon, q_table, env, max_env_steps):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    \n",
    "    for step in range(max_env_steps):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        action = choose_action(q_table, state, epsilon, env)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory, total_reward\n",
    "\n",
    "# Off-Policy Monte Carlo with Importance Sampling\n",
    "def every_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon):\n",
    "    q_table = initialize_q_table(env)\n",
    "    c_table = np.zeros_like(q_table)  # C table for accumulating weights\n",
    "    rewards = []\n",
    "    \n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(total_train_episodes):\n",
    "        epsilon = decay_function(episode, total_train_episodes, min_epsilon)\n",
    "        trajectory, total_reward = generate_episode(epsilon, q_table, env, max_env_steps)\n",
    "        g = 0\n",
    "        w = 1  # Initial weight\n",
    "        \n",
    "        for t in reversed(range(len(trajectory))):\n",
    "            state, action, reward = trajectory[t]\n",
    "            g = gamma * g + reward\n",
    "            c_table[state, action] += w\n",
    "            q_table[state, action] += (w / c_table[state, action]) * (g - q_table[state, action])\n",
    "            \n",
    "            # If the action taken is not the one that would have been taken by the greedy policy, break\n",
    "            if action != np.argmax(q_table[state]):\n",
    "                break\n",
    "            \n",
    "            w *= 1.0 / (epsilon / env.action_space.n)  # Update weight\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            rewards.append(total_reward)\n",
    "            print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    return q_table, rewards\n",
    "\n",
    "\n",
    "# First-Visit Monte Carlo with Off-Policy Control\n",
    "def first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon):\n",
    "    q_table = initialize_q_table(env)\n",
    "    returns_count = np.zeros_like(q_table)  # Counter for returns\n",
    "    rewards = []\n",
    "\n",
    "    # Inizializza un array 3D per memorizzare i valori di ciascuna cella per ogni episodio\n",
    "    history = np.zeros((total_train_episodes, *q_table.shape))\n",
    "\n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(total_train_episodes):\n",
    "        epsilon = decay_function(episode, total_train_episodes, min_epsilon)\n",
    "        trajectory, total_reward = generate_episode(epsilon, q_table, env, max_env_steps)\n",
    "        g = 0\n",
    "        \n",
    "        first_visit_check = set()  # Set to check first visit\n",
    "        \n",
    "        for t in reversed(range(len(trajectory))):\n",
    "            state, action, reward = trajectory[t]\n",
    "            g = gamma * g + reward\n",
    "            \n",
    "            # First visit check\n",
    "            if (state, action) not in first_visit_check:\n",
    "                first_visit_check.add((state, action))\n",
    "                returns_count[state, action] += 1\n",
    "                q_table[state, action] += (1 / returns_count[state, action]) * (g - q_table[state, action])\n",
    "            \n",
    "            # Se l'azione eseguita non è quella che sarebbe stata scelta dalla policy greedy, interrompi\n",
    "            if action != np.argmax(q_table[state]):\n",
    "                break\n",
    "\n",
    "        # Memorizza lo stato della Q-table al termine dell'episodio\n",
    "        history[episode] = q_table\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            rewards.append(total_reward)\n",
    "            print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    \n",
    "    # Calcola la varianza per ciascuna cella attraverso tutte le iterazioni\n",
    "    varianza_per_cella = np.var(history, axis=0)\n",
    "    print(f\"\\nVarianza per ogni cella della Q-table:\\n{varianza_per_cella}\\n\")\n",
    "    plot_heatmap(varianza_per_cella)\n",
    "    plot_matrix_colormap(varianza_per_cella)\n",
    "    plot_bar_chart(varianza_per_cella)\n",
    "    plot_surface(varianza_per_cella)\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    return q_table, rewards\n",
    "\n",
    "\n",
    "# Plotting the training rewards\n",
    "def plot_rewards(rewards, title, save_path):\n",
    "    x = np.linspace(0, len(rewards) * 50, len(rewards))\n",
    "    plt.plot(x, rewards, label='Monte Carlo')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Testing the policy with an epsilon-greedy approach\n",
    "def test_policy_epsilon(env, q_table, num_episodes=10, epsilon=0.1):\n",
    "    print(f\"Testing the policy with epsilon = {epsilon}\")\n",
    "    rewards = []\n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_env_steps):\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            \n",
    "            # Use epsilon-greedy for testing\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Exploration\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])  # Exploitation\n",
    "\n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            env.render()\n",
    "            if done:\n",
    "                env.render()\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_rewards)\n",
    "        print(f\"Episode {episode}, reward {total_rewards}\")\n",
    "    \n",
    "    print(f\"Average score: {np.mean(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:54:50.454860Z",
     "start_time": "2024-08-28T09:54:50.437676Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(varianza_per_cella):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(varianza_per_cella, annot=True, cmap=\"Reds\", cbar=True, square=True)\n",
    "    plt.title(\"Mappa di Calore della Varianza per Cella della Q-Table\")\n",
    "    plt.xlabel(\"Azioni (Sinistra, Giù, Destra, Su)\")\n",
    "    plt.ylabel(\"Stati\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_matrix_colormap(varianza_per_cella):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(varianza_per_cella, cmap='Greys', interpolation='none')\n",
    "    plt.colorbar(label='Varianza')\n",
    "    plt.title(\"Varianza della Q-Table (16x4)\")\n",
    "    plt.xlabel(\"Azioni (Sinistra, Giù, Destra, Su)\")\n",
    "    plt.ylabel(\"Celle\")\n",
    "    plt.xticks(ticks=[0, 1, 2, 3], labels=['Sinistra', 'Giù', 'Destra', 'Su'])\n",
    "    plt.yticks(ticks=np.arange(16), labels=[f\"Cella {i+1}\" for i in range(16)])\n",
    "    plt.show()\n",
    "\n",
    "def plot_bar_chart(varianza_per_cella):\n",
    "    varianza_flat = varianza_per_cella.flatten()\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(range(len(varianza_flat)), varianza_flat, color='blue')\n",
    "    plt.title(\"Varianza per Stato-Azione nella Q-Table\")\n",
    "    plt.xlabel(\"Combinazioni Stato-Azione\")\n",
    "    plt.ylabel(\"Varianza\")\n",
    "    plt.show()\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_surface(varianza_per_cella):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    X, Y = np.meshgrid(range(varianza_per_cella.shape[1]), range(varianza_per_cella.shape[0]))\n",
    "    ax.plot_surface(X, Y, varianza_per_cella, cmap='viridis')\n",
    "    ax.set_title(\"Superficie 3D della Varianza per Cella\")\n",
    "    ax.set_xlabel(\"Azioni\")\n",
    "    ax.set_ylabel(\"Stati\")\n",
    "    ax.set_zlabel(\"Varianza\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:54:59.982531Z",
     "start_time": "2024-08-28T09:54:51.778584Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Main code\n",
    "# if __name__ == \"__main__\":\n",
    "#     n_times = 1 #10 is is slippery true, 5 altrimenti\n",
    "#     total_train_episodes = 1000 #50000 if is slippery true\n",
    "#     gamma = 0.99\n",
    "#     max_epsilon = 1.0\n",
    "#     min_epsilon = 0.01\n",
    "\n",
    "#     env = create_env()\n",
    "\n",
    "#     MC_tables = []\n",
    "#     MC_rewards = []\n",
    "\n",
    "#     for number in range(n_times):\n",
    "#         print(f\"\\n ********** Training number {number}\")\n",
    "#         q_table, rewards = first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "#         MC_tables.append(q_table)\n",
    "#         MC_rewards.append(rewards)\n",
    "        \n",
    "#     #Da non considerare mc mean rw    \n",
    "#     MC_mean = np.mean(np.array(MC_rewards), axis=0)\n",
    "#     MC_mean_tables = np.mean(np.array(MC_tables), axis=0)\n",
    "    \n",
    "#     np.save('OFF/imgs/FV_MC_mean.npy', MC_mean)\n",
    "#     np.save('OFF/models/FV_MC_tables.npy', MC_mean_tables)\n",
    "#     #np.save('OFF/imgs/FV_MC_IS_mean.npy', MC_mean)\n",
    "#     #np.save('OFF/models/FV_MC_IS_tables.npy', MC_mean_tables)\n",
    "#     #EV\n",
    "#     #np.save('OFF/imgs/EV_MC_mean.npy', MC_mean)\n",
    "#     #np.save('OFF/models/EV_MC_tables.npy', MC_mean_tables)\n",
    "#     #np.save('OFF/imgs/EV_MC_IS_mean.npy', MC_mean)\n",
    "#     #np.save('OFF/models/EV_MC_IS_tables.npy', MC_mean_tables)\n",
    "#     print(\"Mean rewards: \", MC_mean_tables)\n",
    "\n",
    "#     # plot_rewards(MC_mean, \"Off-Policy Monte Carlo FV\", \"OFF/imgs/FV_MC_mean.png\")\n",
    "#     # plot_rewards(MC_mean, \"Off-Policy Monte Carlo FV\", \"OFF/imgs/FV_MC_IS_mean.png\")\n",
    "#     #plot_rewards(MC_mean, \"Off-Policy Monte Carlo FV\", \"OFF/imgs/EV_MC_IS_mean.png\")\n",
    "\n",
    "#     # Load Q-tables and test\n",
    "#     q_table = np.load('OFF/models/FV_MC_tables.npy', allow_pickle=True)\n",
    "#     #q_table = np.load('OFF/models/FV_MC_IS_tables.npy', allow_pickle=True)\n",
    "#     # q_table = np.load('OFF/models/EV_MC_tables.npy', allow_pickle=True)\n",
    "#     #q_table = np.load('OFF/models/EV_MC_IS_tables.npy', allow_pickle=True)\n",
    "    \n",
    "#     env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "    \n",
    "#     test_policy_epsilon(env, q_table, num_episodes=1, epsilon=0.1)\n",
    "    \n",
    "#     env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T15:20:32.016317Z",
     "start_time": "2024-08-27T15:20:21.551008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Training number 0\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 1.0000, reward 0.00\n",
      "Episode 200, epsilon 1.0000, reward 0.00\n",
      "Episode 300, epsilon 1.0000, reward 0.00\n",
      "Episode 400, epsilon 1.0000, reward 0.00\n",
      "Episode 500, epsilon 0.9991, reward 0.00\n",
      "Episode 600, epsilon 0.9201, reward 0.00\n",
      "Episode 700, epsilon 0.8533, reward 0.00\n",
      "Episode 800, epsilon 0.7953, reward 1.00\n",
      "Episode 900, epsilon 0.7442, reward 0.00\n",
      "Episode 1000, epsilon 0.6985, reward 1.00\n",
      "Episode 1100, epsilon 0.6572, reward 0.00\n",
      "Episode 1200, epsilon 0.6194, reward 0.00\n",
      "Episode 1300, epsilon 0.5847, reward 0.00\n",
      "Episode 1400, epsilon 0.5525, reward 0.00\n",
      "Episode 1500, epsilon 0.5226, reward 0.00\n",
      "Episode 1600, epsilon 0.4946, reward 0.00\n",
      "Episode 1700, epsilon 0.4683, reward 0.00\n",
      "Episode 1800, epsilon 0.4435, reward 0.00\n",
      "Episode 1900, epsilon 0.4200, reward 0.00\n",
      "Episode 2000, epsilon 0.3977, reward 0.00\n",
      "Episode 2100, epsilon 0.3765, reward 0.00\n",
      "Episode 2200, epsilon 0.3563, reward 0.00\n",
      "Episode 2300, epsilon 0.3371, reward 1.00\n",
      "Episode 2400, epsilon 0.3186, reward 1.00\n",
      "Episode 2500, epsilon 0.3009, reward 0.00\n",
      "Episode 2600, epsilon 0.2838, reward 1.00\n",
      "Episode 2700, epsilon 0.2674, reward 1.00\n",
      "Episode 2800, epsilon 0.2517, reward 0.00\n",
      "Episode 2900, epsilon 0.2364, reward 1.00\n",
      "Episode 3000, epsilon 0.2217, reward 1.00\n",
      "Episode 3100, epsilon 0.2075, reward 1.00\n",
      "Episode 3200, epsilon 0.1937, reward 1.00\n",
      "Episode 3300, epsilon 0.1803, reward 1.00\n",
      "Episode 3400, epsilon 0.1674, reward 1.00\n",
      "Episode 3500, epsilon 0.1548, reward 1.00\n",
      "Episode 3600, epsilon 0.1425, reward 1.00\n",
      "Episode 3700, epsilon 0.1307, reward 0.00\n",
      "Episode 3800, epsilon 0.1191, reward 1.00\n",
      "Episode 3900, epsilon 0.1078, reward 0.00\n",
      "Episode 4000, epsilon 0.0968, reward 1.00\n",
      "Episode 4100, epsilon 0.0861, reward 1.00\n",
      "Episode 4200, epsilon 0.0756, reward 0.00\n",
      "Episode 4300, epsilon 0.0654, reward 1.00\n",
      "Episode 4400, epsilon 0.0554, reward 1.00\n",
      "Episode 4500, epsilon 0.0457, reward 1.00\n",
      "Episode 4600, epsilon 0.0361, reward 1.00\n",
      "Episode 4700, epsilon 0.0268, reward 1.00\n",
      "Episode 4800, epsilon 0.0176, reward 1.00\n",
      "Episode 4900, epsilon 0.0100, reward 1.00\n",
      "Episode 4999, epsilon 0.0100, reward 1.00\n",
      "\n",
      " ********** Training number 1\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 1.0000, reward 0.00\n",
      "Episode 200, epsilon 1.0000, reward 0.00\n",
      "Episode 300, epsilon 1.0000, reward 0.00\n",
      "Episode 400, epsilon 1.0000, reward 0.00\n",
      "Episode 500, epsilon 0.9991, reward 0.00\n",
      "Episode 600, epsilon 0.9201, reward 0.00\n",
      "Episode 700, epsilon 0.8533, reward 0.00\n",
      "Episode 800, epsilon 0.7953, reward 0.00\n",
      "Episode 900, epsilon 0.7442, reward 0.00\n",
      "Episode 1000, epsilon 0.6985, reward 0.00\n",
      "Episode 1100, epsilon 0.6572, reward 0.00\n",
      "Episode 1200, epsilon 0.6194, reward 0.00\n",
      "Episode 1300, epsilon 0.5847, reward 0.00\n",
      "Episode 1400, epsilon 0.5525, reward 0.00\n",
      "Episode 1500, epsilon 0.5226, reward 0.00\n",
      "Episode 1600, epsilon 0.4946, reward 1.00\n",
      "Episode 1700, epsilon 0.4683, reward 0.00\n",
      "Episode 1800, epsilon 0.4435, reward 1.00\n",
      "Episode 1900, epsilon 0.4200, reward 1.00\n",
      "Episode 2000, epsilon 0.3977, reward 0.00\n",
      "Episode 2100, epsilon 0.3765, reward 1.00\n",
      "Episode 2200, epsilon 0.3563, reward 1.00\n",
      "Episode 2300, epsilon 0.3371, reward 0.00\n",
      "Episode 2400, epsilon 0.3186, reward 1.00\n",
      "Episode 2500, epsilon 0.3009, reward 0.00\n",
      "Episode 2600, epsilon 0.2838, reward 1.00\n",
      "Episode 2700, epsilon 0.2674, reward 1.00\n",
      "Episode 2800, epsilon 0.2517, reward 0.00\n",
      "Episode 2900, epsilon 0.2364, reward 1.00\n",
      "Episode 3000, epsilon 0.2217, reward 0.00\n",
      "Episode 3100, epsilon 0.2075, reward 1.00\n",
      "Episode 3200, epsilon 0.1937, reward 1.00\n",
      "Episode 3300, epsilon 0.1803, reward 1.00\n",
      "Episode 3400, epsilon 0.1674, reward 1.00\n",
      "Episode 3500, epsilon 0.1548, reward 0.00\n",
      "Episode 3600, epsilon 0.1425, reward 0.00\n",
      "Episode 3700, epsilon 0.1307, reward 1.00\n",
      "Episode 3800, epsilon 0.1191, reward 0.00\n",
      "Episode 3900, epsilon 0.1078, reward 1.00\n",
      "Episode 4000, epsilon 0.0968, reward 1.00\n",
      "Episode 4100, epsilon 0.0861, reward 1.00\n",
      "Episode 4200, epsilon 0.0756, reward 1.00\n",
      "Episode 4300, epsilon 0.0654, reward 1.00\n",
      "Episode 4400, epsilon 0.0554, reward 1.00\n",
      "Episode 4500, epsilon 0.0457, reward 1.00\n",
      "Episode 4600, epsilon 0.0361, reward 1.00\n",
      "Episode 4700, epsilon 0.0268, reward 1.00\n",
      "Episode 4800, epsilon 0.0176, reward 1.00\n",
      "Episode 4900, epsilon 0.0100, reward 1.00\n",
      "Episode 4999, epsilon 0.0100, reward 1.00\n",
      "\n",
      " ********** Training number 2\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 1.0000, reward 0.00\n",
      "Episode 200, epsilon 1.0000, reward 0.00\n",
      "Episode 300, epsilon 1.0000, reward 0.00\n",
      "Episode 400, epsilon 1.0000, reward 0.00\n",
      "Episode 500, epsilon 0.9991, reward 0.00\n",
      "Episode 600, epsilon 0.9201, reward 0.00\n",
      "Episode 700, epsilon 0.8533, reward 0.00\n",
      "Episode 800, epsilon 0.7953, reward 0.00\n",
      "Episode 900, epsilon 0.7442, reward 0.00\n",
      "Episode 1000, epsilon 0.6985, reward 0.00\n",
      "Episode 1100, epsilon 0.6572, reward 0.00\n",
      "Episode 1200, epsilon 0.6194, reward 1.00\n",
      "Episode 1300, epsilon 0.5847, reward 1.00\n",
      "Episode 1400, epsilon 0.5525, reward 0.00\n",
      "Episode 1500, epsilon 0.5226, reward 1.00\n",
      "Episode 1600, epsilon 0.4946, reward 0.00\n",
      "Episode 1700, epsilon 0.4683, reward 1.00\n",
      "Episode 1800, epsilon 0.4435, reward 0.00\n",
      "Episode 1900, epsilon 0.4200, reward 1.00\n",
      "Episode 2000, epsilon 0.3977, reward 0.00\n",
      "Episode 2100, epsilon 0.3765, reward 0.00\n",
      "Episode 2200, epsilon 0.3563, reward 0.00\n",
      "Episode 2300, epsilon 0.3371, reward 1.00\n",
      "Episode 2400, epsilon 0.3186, reward 0.00\n",
      "Episode 2500, epsilon 0.3009, reward 1.00\n",
      "Episode 2600, epsilon 0.2838, reward 1.00\n",
      "Episode 2700, epsilon 0.2674, reward 0.00\n",
      "Episode 2800, epsilon 0.2517, reward 0.00\n",
      "Episode 2900, epsilon 0.2364, reward 1.00\n",
      "Episode 3000, epsilon 0.2217, reward 1.00\n",
      "Episode 3100, epsilon 0.2075, reward 1.00\n",
      "Episode 3200, epsilon 0.1937, reward 0.00\n",
      "Episode 3300, epsilon 0.1803, reward 0.00\n",
      "Episode 3400, epsilon 0.1674, reward 1.00\n",
      "Episode 3500, epsilon 0.1548, reward 1.00\n",
      "Episode 3600, epsilon 0.1425, reward 1.00\n",
      "Episode 3700, epsilon 0.1307, reward 1.00\n",
      "Episode 3800, epsilon 0.1191, reward 1.00\n",
      "Episode 3900, epsilon 0.1078, reward 1.00\n",
      "Episode 4000, epsilon 0.0968, reward 1.00\n",
      "Episode 4100, epsilon 0.0861, reward 1.00\n",
      "Episode 4200, epsilon 0.0756, reward 1.00\n",
      "Episode 4300, epsilon 0.0654, reward 1.00\n",
      "Episode 4400, epsilon 0.0554, reward 0.00\n",
      "Episode 4500, epsilon 0.0457, reward 1.00\n",
      "Episode 4600, epsilon 0.0361, reward 1.00\n",
      "Episode 4700, epsilon 0.0268, reward 1.00\n",
      "Episode 4800, epsilon 0.0176, reward 1.00\n",
      "Episode 4900, epsilon 0.0100, reward 1.00\n",
      "Episode 4999, epsilon 0.0100, reward 1.00\n",
      "\n",
      " ********** Training number 3\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 1.0000, reward 0.00\n",
      "Episode 200, epsilon 1.0000, reward 0.00\n",
      "Episode 300, epsilon 1.0000, reward 0.00\n",
      "Episode 400, epsilon 1.0000, reward 0.00\n",
      "Episode 500, epsilon 0.9991, reward 0.00\n",
      "Episode 600, epsilon 0.9201, reward 0.00\n",
      "Episode 700, epsilon 0.8533, reward 0.00\n",
      "Episode 800, epsilon 0.7953, reward 0.00\n",
      "Episode 900, epsilon 0.7442, reward 0.00\n",
      "Episode 1000, epsilon 0.6985, reward 0.00\n",
      "Episode 1100, epsilon 0.6572, reward 1.00\n",
      "Episode 1200, epsilon 0.6194, reward 1.00\n",
      "Episode 1300, epsilon 0.5847, reward 0.00\n",
      "Episode 1400, epsilon 0.5525, reward 0.00\n",
      "Episode 1500, epsilon 0.5226, reward 1.00\n",
      "Episode 1600, epsilon 0.4946, reward 0.00\n",
      "Episode 1700, epsilon 0.4683, reward 1.00\n",
      "Episode 1800, epsilon 0.4435, reward 0.00\n",
      "Episode 1900, epsilon 0.4200, reward 0.00\n",
      "Episode 2000, epsilon 0.3977, reward 1.00\n",
      "Episode 2100, epsilon 0.3765, reward 0.00\n",
      "Episode 2200, epsilon 0.3563, reward 1.00\n",
      "Episode 2300, epsilon 0.3371, reward 1.00\n",
      "Episode 2400, epsilon 0.3186, reward 1.00\n",
      "Episode 2500, epsilon 0.3009, reward 0.00\n",
      "Episode 2600, epsilon 0.2838, reward 1.00\n",
      "Episode 2700, epsilon 0.2674, reward 1.00\n",
      "Episode 2800, epsilon 0.2517, reward 1.00\n",
      "Episode 2900, epsilon 0.2364, reward 1.00\n",
      "Episode 3000, epsilon 0.2217, reward 1.00\n",
      "Episode 3100, epsilon 0.2075, reward 1.00\n",
      "Episode 3200, epsilon 0.1937, reward 0.00\n",
      "Episode 3300, epsilon 0.1803, reward 1.00\n",
      "Episode 3400, epsilon 0.1674, reward 1.00\n",
      "Episode 3500, epsilon 0.1548, reward 1.00\n",
      "Episode 3600, epsilon 0.1425, reward 1.00\n",
      "Episode 3700, epsilon 0.1307, reward 1.00\n",
      "Episode 3800, epsilon 0.1191, reward 1.00\n",
      "Episode 3900, epsilon 0.1078, reward 1.00\n",
      "Episode 4000, epsilon 0.0968, reward 1.00\n",
      "Episode 4100, epsilon 0.0861, reward 1.00\n",
      "Episode 4200, epsilon 0.0756, reward 1.00\n",
      "Episode 4300, epsilon 0.0654, reward 1.00\n",
      "Episode 4400, epsilon 0.0554, reward 1.00\n",
      "Episode 4500, epsilon 0.0457, reward 1.00\n",
      "Episode 4600, epsilon 0.0361, reward 1.00\n",
      "Episode 4700, epsilon 0.0268, reward 0.00\n",
      "Episode 4800, epsilon 0.0176, reward 1.00\n",
      "Episode 4900, epsilon 0.0100, reward 1.00\n",
      "Episode 4999, epsilon 0.0100, reward 1.00\n",
      "\n",
      " ********** Training number 4\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 1.0000, reward 0.00\n",
      "Episode 200, epsilon 1.0000, reward 0.00\n",
      "Episode 300, epsilon 1.0000, reward 0.00\n",
      "Episode 400, epsilon 1.0000, reward 0.00\n",
      "Episode 500, epsilon 0.9991, reward 0.00\n",
      "Episode 600, epsilon 0.9201, reward 0.00\n",
      "Episode 700, epsilon 0.8533, reward 0.00\n",
      "Episode 800, epsilon 0.7953, reward 0.00\n",
      "Episode 900, epsilon 0.7442, reward 0.00\n",
      "Episode 1000, epsilon 0.6985, reward 0.00\n",
      "Episode 1100, epsilon 0.6572, reward 0.00\n",
      "Episode 1200, epsilon 0.6194, reward 0.00\n",
      "Episode 1300, epsilon 0.5847, reward 0.00\n",
      "Episode 1400, epsilon 0.5525, reward 1.00\n",
      "Episode 1500, epsilon 0.5226, reward 0.00\n",
      "Episode 1600, epsilon 0.4946, reward 0.00\n",
      "Episode 1700, epsilon 0.4683, reward 1.00\n",
      "Episode 1800, epsilon 0.4435, reward 1.00\n",
      "Episode 1900, epsilon 0.4200, reward 1.00\n",
      "Episode 2000, epsilon 0.3977, reward 1.00\n",
      "Episode 2100, epsilon 0.3765, reward 0.00\n",
      "Episode 2200, epsilon 0.3563, reward 1.00\n",
      "Episode 2300, epsilon 0.3371, reward 1.00\n",
      "Episode 2400, epsilon 0.3186, reward 1.00\n",
      "Episode 2500, epsilon 0.3009, reward 1.00\n",
      "Episode 2600, epsilon 0.2838, reward 1.00\n",
      "Episode 2700, epsilon 0.2674, reward 1.00\n",
      "Episode 2800, epsilon 0.2517, reward 1.00\n",
      "Episode 2900, epsilon 0.2364, reward 1.00\n",
      "Episode 3000, epsilon 0.2217, reward 1.00\n",
      "Episode 3100, epsilon 0.2075, reward 0.00\n",
      "Episode 3200, epsilon 0.1937, reward 1.00\n",
      "Episode 3300, epsilon 0.1803, reward 0.00\n",
      "Episode 3400, epsilon 0.1674, reward 1.00\n",
      "Episode 3500, epsilon 0.1548, reward 1.00\n",
      "Episode 3600, epsilon 0.1425, reward 1.00\n",
      "Episode 3700, epsilon 0.1307, reward 1.00\n",
      "Episode 3800, epsilon 0.1191, reward 1.00\n",
      "Episode 3900, epsilon 0.1078, reward 1.00\n",
      "Episode 4000, epsilon 0.0968, reward 1.00\n",
      "Episode 4100, epsilon 0.0861, reward 1.00\n",
      "Episode 4200, epsilon 0.0756, reward 1.00\n",
      "Episode 4300, epsilon 0.0654, reward 1.00\n",
      "Episode 4400, epsilon 0.0554, reward 1.00\n",
      "Episode 4500, epsilon 0.0457, reward 1.00\n",
      "Episode 4600, epsilon 0.0361, reward 1.00\n",
      "Episode 4700, epsilon 0.0268, reward 1.00\n",
      "Episode 4800, epsilon 0.0176, reward 1.00\n",
      "Episode 4900, epsilon 0.0100, reward 1.00\n",
      "Episode 4999, epsilon 0.0100, reward 1.00\n",
      "Mean rewards:  [[0.94148015 0.95082371 0.75669834 0.94148015]\n",
      " [0.94148015 0.         0.67090779 0.56554744]\n",
      " [0.56484767 0.86416176 0.28015653 0.76847681]\n",
      " [0.57443641 0.         0.37849404 0.38039602]\n",
      " [0.95099005 0.96054881 0.         0.74926129]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.3842384 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.97025999 0.95099005]\n",
      " [0.96059601 0.9531232  0.9801     0.        ]\n",
      " [0.970299   0.98991631 0.         0.94890391]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.91559784 0.988875   0.970299  ]\n",
      " [0.96762746 0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Testing the policy with epsilon = 0.1\n",
      "Episode 0, reward 1.0\n",
      "Average score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    # Caso is slippery=True -> 10, 5 a false\n",
    "    n_times = 5\n",
    "    # Caso is slippery=True -> 50000, 10000 a false\n",
    "    total_train_episodes = 5000\n",
    "    gamma = 0.99\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    env = create_env()\n",
    "\n",
    "    MC_tables = []\n",
    "    MC_rewards = []\n",
    "\n",
    "    for number in range(n_times):\n",
    "        print(f\"\\n ********** Training number {number}\")\n",
    "        q_table, rewards = first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        MC_tables.append(q_table)\n",
    "        MC_rewards.append(rewards)\n",
    "\n",
    "    MC_mean = np.mean(np.array(MC_rewards), axis=0)\n",
    "    MC_mean_tables = np.mean(np.array(MC_tables), axis=0)\n",
    "    #np.save('OFF/imgs/EV_MC_mean.npy', MC_mean)\n",
    "    #np.save('OFF/models/EV_MC_tables.npy', MC_mean_tables) \n",
    "    #np.save('OFF/imgs/EV_MC_IS_mean.npy', MC_mean)\n",
    "    #np.save('OFF/models/EV_MC_IS_tables.npy', MC_mean_tables)\n",
    "    np.save('OFF/imgs/FV_MC_mean.npy', MC_mean)\n",
    "    np.save('OFF/models/FV_MC_tables.npy', MC_mean_tables) \n",
    "    #np.save('OFF/imgs/FV_MC_IS_mean.npy', MC_mean)\n",
    "    #np.save('OFF/models/FV_MC_IS_tables.npy', MC_mean_tables)\n",
    "    print(\"Mean rewards: \", MC_mean_tables)\n",
    "\n",
    "    #plot_rewards(MC_mean, \"Off-Policy Monte Carlo EV\", \"OFF/imgs/EV_MC_mean.png\") MISSING!!!!!!!!!!!!!!\n",
    "    #plot_rewards(MC_mean, \"Off-Policy Monte Carlo EV\", \"OFF/imgs/EV_MC_IS_mean.png\")\n",
    "    #plot_rewards(MC_mean, \"Off-Policy Monte Carlo FV\", \"OFF/imgs/FV_MC_mean.png\")\n",
    "    #plot_rewards(MC_mean, \"Off-Policy Monte Carlo FV\", \"OFF/imgs/FV_MC_IS_mean.png\")\n",
    "\n",
    "    # Load Q-tables and test\n",
    "    #q_table = np.load('OFF/models/EV_MC_IS_tables.npy', allow_pickle=True)\n",
    "    #q_table = np.load('OFF/models/EV_MC_tables.npy', allow_pickle=True)\n",
    "    q_table = np.load('OFF/models/FV_MC_tables.npy', allow_pickle=True)\n",
    "    #q_table = np.load('OFF/models/FV_MC_IS_tables.npy', allow_pickle=True)\n",
    "    \n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "    \n",
    "    test_policy_epsilon(env, q_table, num_episodes=1, epsilon=0.1)\n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_policy(q_table, grid_size=(4, 4), title='Policy', save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the policy learned by the Q-table on a grid using text labels for directions.\n",
    "\n",
    "    Parameters:\n",
    "        q_table (numpy.ndarray): The Q-table from which to derive the policy.\n",
    "        grid_size (tuple): The size of the grid representing the environment (default is (4, 4)).\n",
    "    \"\"\"\n",
    "    # Define direction labels\n",
    "    direction_labels = {\n",
    "        0: '←',  # left\n",
    "        1: '↓',  # down\n",
    "        2: '→',  # right\n",
    "        3: '↑'   # up\n",
    "    }\n",
    "\n",
    "  \n",
    "    policy = np.argmax(q_table, axis=1).reshape(grid_size)\n",
    "    \n",
    "    \n",
    "    # Set up the grid\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(np.arange(grid_size[1]))\n",
    "    ax.set_yticks(np.arange(grid_size[0]))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    # Create grid\n",
    "    ax.grid(which='both', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Plot direction labels\n",
    "    i=0\n",
    "    j=0\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            action = policy[i, j]\n",
    "            direction = direction_labels[action]\n",
    "            #print(\"Cella:\",i,j, \"Azione\", action, \"Direzione\", direction)\n",
    "            ax.text(j + 0.5, grid_size[0] - i - 0.5, direction,\n",
    "                    ha='center', va='center', fontsize=12, color='black')\n",
    "\n",
    "    # Set the aspect of the plot to equal\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, grid_size[1])\n",
    "    ax.set_ylim(0, grid_size[0])\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have already saved your Q-tables as 'MC_tables.npy'\n",
    "    #FV_q_table = np.load('OFF/models/FV_MC_tables.npy', allow_pickle=True)\n",
    "    #EV_q_table = np.load('OFF/models/EV_MC_tables.npy', allow_pickle=True)\n",
    "    #Prima erano FV_policy e EV_policy, poi abbiamo cambiato in FV_IS e EV_IS\n",
    "    #plot_policy(FV_q_table, title='FV Off Policy', save_path='OFF/imgs/FV_policy.png')\n",
    "    #plot_policy(EV_q_table, title='EV Policy', save_path='OFF/imgs/EV_policy.png')\n",
    "    #EV_q_table = np.load('OFF/models/EV_MC_IS_tables.npy', allow_pickle=True)\n",
    "    #EV_q_table = np.load('OFF/models/EV_MC_IS_tables.npy', allow_pickle=True)    \n",
    "    #plot_policy(EV_q_table, title='EV Off Policy', save_path='OFF/imgs/EV_MC_policy.png')\n",
    "    #plot_policy(EV_q_table, title='EV Off Policy', save_path='OFF/imgs/EV_MC_IS_policy.png')\n",
    "    \n",
    "    #FV_q_table = np.load('OFF/models/FV_MC_tables.npy', allow_pickle=True)\n",
    "    FV_q_table = np.load('OFF/models/FV_MC_IS_tables.npy', allow_pickle=True)    \n",
    "    plot_policy(FV_q_table, title='FV Off Policy', save_path='OFF/imgs/FV_MC_IS_policy.png')\n",
    "    #plot_policy(FV_q_table, title='FV Off Policy', save_path='OFF/imgs/FV_MC_policy.png')\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codice corretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T17:46:09.612073Z",
     "start_time": "2024-08-31T17:46:09.562910Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment\n",
    "def create_env():\n",
    "    return gym.make('FrozenLake-v1', is_slippery=False)  # or True, depending on your use case\n",
    "\n",
    "# Initialize Q-table\n",
    "def initialize_q_table(env):\n",
    "    number_of_states = env.observation_space.n #16, campo 4x4\n",
    "    number_of_actions = env.action_space.n #4, su,giu,dx,sx\n",
    "    return np.zeros((number_of_states, number_of_actions))\n",
    "\n",
    "# Decay function for epsilon\n",
    "def decay_function(episode, total_train_episodes, min_epsilon=0.01):\n",
    "    return max(min_epsilon, min(1.0, 1.0 - np.log10((episode + 1) / (total_train_episodes * 0.1))))\n",
    "\n",
    "# Choose action based on epsilon-greedy policy (Behavior Policy)\n",
    "def choose_action(q_table, state, epsilon, env):\n",
    "    if np.random.random() <= epsilon:\n",
    "        return env.action_space.sample()  # Exploration\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploitation\n",
    "\n",
    "# Generate an episode\n",
    "def generate_episode(epsilon, q_table, env, max_env_steps):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    \n",
    "    for step in range(max_env_steps):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        action = choose_action(q_table, state, epsilon, env)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory, total_reward\n",
    "\n",
    "# Off-Policy Monte Carlo with Importance Sampling\n",
    "\n",
    "def every_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon):\n",
    "    q_table = initialize_q_table(env)\n",
    "    c_table = np.zeros_like(q_table)  # C table for accumulating weights\n",
    "    rewards = []\n",
    "    success_counter = 0\n",
    "    differences = []\n",
    "    \n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    stop = False\n",
    "    \n",
    "    for episode in range(total_train_episodes):\n",
    "        if not stop:\n",
    "            epsilon = decay_function(episode, total_train_episodes, min_epsilon)\n",
    "            trajectory, total_reward = generate_episode(epsilon, q_table, env, max_env_steps)\n",
    "            g = 0\n",
    "            w = 1  # Initial weight\n",
    "            \n",
    "            for t in reversed(range(len(trajectory))):\n",
    "                state, action, reward = trajectory[t]\n",
    "                if reward==1:\n",
    "                    success_counter +=1\n",
    "                g = gamma * g + reward\n",
    "                c_table[state, action] += w\n",
    "                q_table[state, action] += (w / c_table[state, action]) * (g - q_table[state, action])\n",
    "                differences.append(q_table[9,1])\n",
    "                \n",
    "                # If the action taken is not the one that would have been taken by the greedy policy, break\n",
    "                if action != np.argmax(q_table[state]):\n",
    "                    break\n",
    "                \n",
    "                w *= 1.0 / (epsilon / env.action_space.n)  # Update weight\n",
    "            \n",
    "            if episode % 50 == 0:\n",
    "                rewards.append(total_reward)\n",
    "                print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "            if success_counter == 10:\n",
    "                print(f\"\\n***** Convergence reached at episode {episode} *****\\n\")\n",
    "                with open('PROVE/convergence_log.txt', 'a') as file:\n",
    "                    file.write(f\"Ex number: {number}: Convergence: {episode} Epsilon: {epsilon}\\n\")\n",
    "                stop = True\n",
    "            if episode==total_train_episodes-1:\n",
    "                with open('PROVE/convergence_log.txt', 'a') as file:\n",
    "                    file.write(f\"Ex number: {number} Convergence: NO\\n\")\n",
    "    \n",
    "    #rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    return q_table, rewards, differences\n",
    "\n",
    "\n",
    "# First-Visit Monte Carlo with Off-Policy Control\n",
    "def first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon):\n",
    "    q_table = initialize_q_table(env)\n",
    "    returns_count = np.zeros_like(q_table)  # Counter for returns\n",
    "    rewards = []\n",
    "    success_counter=0\n",
    "    stop = False\n",
    "    differences = []\n",
    "\n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(total_train_episodes):\n",
    "        if not stop:\n",
    "            epsilon = decay_function(episode, total_train_episodes, min_epsilon)\n",
    "            trajectory, total_reward = generate_episode(epsilon, q_table, env, max_env_steps)\n",
    "            g = 0\n",
    "            \n",
    "            first_visit_check = set()  # Set to check first visit\n",
    "            \n",
    "            for t in reversed(range(len(trajectory))):\n",
    "                state, action, reward = trajectory[t]\n",
    "                if reward==1:\n",
    "                    success_counter +=1\n",
    "                g = gamma * g + reward\n",
    "                \n",
    "                # First visit check\n",
    "                if (state, action) not in first_visit_check:\n",
    "                    first_visit_check.add((state, action))\n",
    "                    returns_count[state, action] += 1\n",
    "                    q_table[state, action] += (1 / returns_count[state, action]) * (g - q_table[state, action])\n",
    "                    differences.append(q_table[0,1])\n",
    "                \n",
    "                # Se l'azione eseguita non è quella che sarebbe stata scelta dalla policy greedy, interrompi\n",
    "                if action != np.argmax(q_table[state]):\n",
    "                    break\n",
    "            \n",
    "            if episode % 50 == 0:\n",
    "                rewards.append(total_reward)\n",
    "                print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "            if success_counter == 10:\n",
    "                print(f\"\\n***** Convergence reached at episode {episode} *****\\n\")\n",
    "                with open('final/off/first_visit/convergence_log.txt', 'a') as file:\n",
    "                    file.write(f\"Ex number: {number}: Convergence: {episode} Epsilon: {epsilon}\\n\")\n",
    "                stop = True\n",
    "            if episode==total_train_episodes-1:\n",
    "                with open('final/off/first_visit/convergence_log.txt', 'a') as file:\n",
    "                    file.write(f\"Ex number: {number} Convergence: NO\\n\")\n",
    "    \n",
    "    #print(f\"\\nVarianza per ogni cella della Q-table:\\n{varianza_per_cella}\\n\")\n",
    "    #rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, epsilon {epsilon:.4f}, reward {total_reward:.2f}\")\n",
    "    #plot_heatmap(varianza_per_cella)\n",
    "    #plot_matrix_colormap(varianza_per_cella)\n",
    "    #plot_bar_chart(varianza_per_cella)\n",
    "    #plot_surface(varianza_per_cella)\n",
    "    return q_table, rewards, differences\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the training rewards\n",
    "def plot_rewards(rewards, title, save_path):\n",
    "    x = np.linspace(0, len(rewards) * 50, len(rewards))\n",
    "    plt.plot(x, rewards, label='Monte Carlo')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Testing the policy with an epsilon-greedy approach\n",
    "def test_policy_epsilon(env, q_table, num_episodes=10, epsilon=0.1):\n",
    "    print(f\"Testing the policy with epsilon = {epsilon}\")\n",
    "    rewards = []\n",
    "    max_env_steps = env.spec.max_episode_steps\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_env_steps):\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            \n",
    "            # Use epsilon-greedy for testing\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Exploration\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])  # Exploitation\n",
    "\n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            env.render()\n",
    "            if done:\n",
    "                env.render()\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_rewards)\n",
    "        print(f\"Episode {episode}, reward {total_rewards}\")\n",
    "    \n",
    "    print(f\"Average score: {np.mean(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T17:46:14.878261Z",
     "start_time": "2024-08-31T17:46:11.043019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Training number 0\n",
      "Episode 0, epsilon 1.0000, reward 0.00\n",
      "Episode 50, epsilon 1.0000, reward 0.00\n",
      "Episode 100, epsilon 0.9957, reward 0.00\n",
      "Episode 150, epsilon 0.8210, reward 0.00\n",
      "Episode 200, epsilon 0.6968, reward 1.00\n",
      "\n",
      "***** Convergence reached at episode 201 *****\n",
      "\n",
      "Episode 999, epsilon 0.6946, reward 1.00\n",
      "\n",
      "\n",
      " DIFFERENCE \n",
      "\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801, 0.9801]\n",
      "Testing the policy with epsilon = 0.1\n",
      "Episode 0, reward 1.0\n",
      "Average score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    n_times = 100 #10 is is slippery true, 5 altrimenti\n",
    "    total_train_episodes = 1000 #50000 if is slippery true\n",
    "    gamma = 0.99\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    env = create_env()\n",
    "\n",
    "    MC_tables = []\n",
    "    MC_rewards = []\n",
    "    \n",
    "    variances = []\n",
    "\n",
    "    for number in range(1):\n",
    "        print(f\"\\n ********** Training number {number}\")\n",
    "        #q_table, rewards = first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        # q_table, rewards = every_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        # q_table, rewards, differences = first_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        q_table, rewards, differences = every_visit_monte_carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon)\n",
    "        # q_table, rewards = Monte_Carlo(env, total_train_episodes, gamma, max_epsilon, min_epsilon,True)\n",
    "        #print(f\"\\n\\n*** DIFFERENCES ***\\n\\n + {differences}\")\n",
    "        MC_tables.append(q_table)\n",
    "        MC_rewards.append(rewards)\n",
    "        \n",
    "    print(f\"\\n\\n DIFFERENCE \\n\\n {differences}\")\n",
    "    \n",
    "    # we perform the mean of the rewards and the Q-tables to reduce variance\n",
    "    #MC_mean = np.mean(np.array(MC_rewards), axis=0)\n",
    "    MC_mean_table = np.mean(np.array(MC_tables), axis=0)\n",
    "    #save_path_rw = 'final/off/every_visit/imgs/results_MC_OFF_TAB_rw.png'  # Adding .npy to this path\n",
    "    #save_path_mc = 'final/off/every_visit/models/table_MC_OFF_TAB.npy'\n",
    "    save_path_rw = 'final/off/first_visit/imgs/results_MC_OFF_TAB_rw.png'  # Adding .npy to this path\n",
    "    save_path_mc = 'final/off/first_visit/models/table_MC_OFF_TAB.npy'\n",
    "    #np.save(save_path_rw, MC_mean)  # Save the mean rewards\n",
    "    np.save(save_path_mc, MC_mean_table)  # Save the mean Q-table\n",
    "    \n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=\"human\")\n",
    "    q_table = np.load(save_path_mc, allow_pickle=True)\n",
    "    test_policy_epsilon(env, q_table, num_episodes=1, epsilon=0.1)\n",
    "    \n",
    "    env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T17:45:43.815423Z",
     "start_time": "2024-08-31T17:45:43.799424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di volte in cui la convergenza è stata raggiunta: 32\n",
      "Numero medio di episodi richiesti: 85.34375\n",
      "Valore medio di epsilon: 0.3788337870644494\n"
     ]
    }
   ],
   "source": [
    " def analyze_convergence_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    count_no_convergence = 0\n",
    "    episodes = []\n",
    "    epsilons = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Convergence: NO\" in line:\n",
    "            count_no_convergence += 1\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                # Trovare il numero di episodi\n",
    "                convergence_index = parts.index('Convergence:') + 1\n",
    "                episodes.append(int(parts[convergence_index]))\n",
    "\n",
    "                # Trovare il valore di epsilon\n",
    "                epsilon_index = parts.index('Epsilon:') + 1\n",
    "                epsilons.append(float(parts[epsilon_index]))\n",
    "            except ValueError:\n",
    "                continue  # salta eventuali linee mal formattate\n",
    "\n",
    "    total_convergence = len(lines) - count_no_convergence\n",
    "    average_episodes = sum(episodes) / len(episodes) if episodes else 0\n",
    "    average_epsilon = sum(epsilons) / len(epsilons) if epsilons else 0\n",
    "\n",
    "    return {\n",
    "        'Numero di volte in cui la convergenza è stata raggiunta': total_convergence,\n",
    "        'Numero medio di episodi richiesti': average_episodes,\n",
    "        'Valore medio di epsilon': average_epsilon\n",
    "    }\n",
    "\n",
    "# Nome del file da leggere\n",
    "filename = 'final/off/every_visit/convergence_log.txt'\n",
    "results = analyze_convergence_data(filename)\n",
    "print(\"Numero di volte in cui la convergenza è stata raggiunta:\", results['Numero di volte in cui la convergenza è stata raggiunta'])\n",
    "print(\"Numero medio di episodi richiesti:\", results['Numero medio di episodi richiesti'])\n",
    "print(\"Valore medio di epsilon:\", results['Valore medio di epsilon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T14:40:23.371818Z",
     "start_time": "2024-08-31T14:40:23.364865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di volte in cui la convergenza è stata raggiunta: 36\n",
      "Numero medio di episodi richiesti: 84.52777777777777\n",
      "Valore medio di epsilon: 0.3839100023471462\n"
     ]
    }
   ],
   "source": [
    "filename = 'final/off/first_visit/convergence_log.txt'\n",
    "results = analyze_convergence_data(filename)\n",
    "print(\"Numero di volte in cui la convergenza è stata raggiunta:\", results['Numero di volte in cui la convergenza è stata raggiunta'])\n",
    "print(\"Numero medio di episodi richiesti:\", results['Numero medio di episodi richiesti'])\n",
    "print(\"Valore medio di epsilon:\", results['Valore medio di epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T14:40:31.691082Z",
     "start_time": "2024-08-31T14:40:31.681489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di volte in cui la convergenza è stata raggiunta: 36\n",
      "Numero medio di episodi richiesti: 82.36111111111111\n",
      "Valore medio di epsilon: 0.39033331738961047\n"
     ]
    }
   ],
   "source": [
    "filename = 'final/on/every_visit/convergence_log.txt'\n",
    "results = analyze_convergence_data(filename)\n",
    "print(\"Numero di volte in cui la convergenza è stata raggiunta:\", results['Numero di volte in cui la convergenza è stata raggiunta'])\n",
    "print(\"Numero medio di episodi richiesti:\", results['Numero medio di episodi richiesti'])\n",
    "print(\"Valore medio di epsilon:\", results['Valore medio di epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T14:40:44.098836Z",
     "start_time": "2024-08-31T14:40:44.092471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di volte in cui la convergenza è stata raggiunta: 40\n",
      "Numero medio di episodi richiesti: 73.3\n",
      "Valore medio di epsilon: 0.43652234009586677\n"
     ]
    }
   ],
   "source": [
    "filename = 'final/on/first_visit/convergence_log.txt'\n",
    "results = analyze_convergence_data(filename)\n",
    "print(\"Numero di volte in cui la convergenza è stata raggiunta:\", results['Numero di volte in cui la convergenza è stata raggiunta'])\n",
    "print(\"Numero medio di episodi richiesti:\", results['Numero medio di episodi richiesti'])\n",
    "print(\"Valore medio di epsilon:\", results['Valore medio di epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cidl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

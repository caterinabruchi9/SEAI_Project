{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MonteCarloTreeSearch:\n",
    "#     def __init__(self, env, V, initial_state=None, max_steps=2):\n",
    "#         self.env = env\n",
    "#         self.V = V  # Value matrix\n",
    "#         self.action_space = self.env.action_space.n\n",
    "#         self.max_steps = max_steps  # Limit search depth to 2 steps\n",
    "#         self.initial_state = initial_state\n",
    "#         self.reset_tree()\n",
    "\n",
    "#     def reset_tree(self):\n",
    "#         \"\"\"Reset the tree with the given state as the root.\"\"\"\n",
    "#         if self.initial_state is None:\n",
    "#             state = self.env.reset()[0]\n",
    "#         else:\n",
    "#             self.env.reset()  # Reset the environment to the start.\n",
    "#             self.env.unwrapped.s = self.initial_state  # Manually set the state to initial_state.\n",
    "#             state = self.initial_state\n",
    "        \n",
    "#         root_node = Node(state=state, action=None, action_space=self.action_space, reward=0, terminal=False)\n",
    "#         self.tree = Tree()\n",
    "#         self.tree.add_node(root_node)\n",
    "\n",
    "#     def expand(self, node):\n",
    "#         action = node.untried_action()\n",
    "        \n",
    "#         # Save the environment's state before taking the action\n",
    "#         previous_state = self.env.unwrapped.s\n",
    "        \n",
    "#         state, _, done, _, _ = self.env.step(action)\n",
    "#         new_node = Node(state=state, action=action, action_space=self.action_space, reward=self.V[state], terminal=done)\n",
    "#         self.tree.add_node(new_node, node)\n",
    "\n",
    "#         # Restore the environment's state\n",
    "#         self.env.unwrapped.s = previous_state\n",
    "\n",
    "#         return new_node\n",
    "\n",
    "#     def default_policy(self, node):\n",
    "#         # Ensure the environment is set to the state of the node before the simulation\n",
    "#         state = node.state\n",
    "#         total_reward = 0\n",
    "#         done = node.terminal\n",
    "\n",
    "#         # Manually set the environment to the node's state\n",
    "#         self.env.unwrapped.s = state\n",
    "        \n",
    "#         print(f\"Starting simulation from state: {state}\")\n",
    "\n",
    "#         # First step\n",
    "#         if not done:\n",
    "#             action = random.choice(range(self.action_space))\n",
    "#             state, reward, done, _, _ = self.env.step(action)\n",
    "#             total_reward += self.V[state]\n",
    "#             print(f\"  Step 1 - Action: {action}, New State: {state}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "#         # Second step\n",
    "#         if not done:\n",
    "#             action = random.choice(range(self.action_space))\n",
    "#             state, reward, done, _, _ = self.env.step(action)\n",
    "#             total_reward += self.V[state]\n",
    "#             print(f\"  Step 2 - Action: {action}, New State: {state}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "#         print(f\"Total reward from simulation: {total_reward}\")\n",
    "#         return total_reward\n",
    "\n",
    "\n",
    "#     def tree_policy(self):\n",
    "#         node = self.tree.root\n",
    "#         depth = 0\n",
    "\n",
    "#         while not node.terminal and depth < self.max_steps:\n",
    "#             if self.tree.is_expandable(node):\n",
    "#                 return self.expand(node)\n",
    "#             else:\n",
    "#                 node = random.choice(self.tree.children(node))  # Move to one of the children\n",
    "#             depth += 1\n",
    "\n",
    "#         return node\n",
    "\n",
    "#     def backward(self, node, value):\n",
    "#         while node:\n",
    "#             node.num_visits += 1\n",
    "#             node.total_simulation_reward += value\n",
    "#             node.performance = node.total_simulation_reward / node.num_visits\n",
    "#             node = self.tree.parent(node)\n",
    "\n",
    "#     def forward(self):\n",
    "#         \"\"\"Perform a single iteration of MCTS with two-level deep simulation as default policy.\"\"\"\n",
    "#         leaf_node = self.tree_policy()\n",
    "#         print(f\"Selected leaf node for expansion: {leaf_node.state}\")\n",
    "#         simulation_result = self.default_policy(leaf_node)\n",
    "#         self.backward(leaf_node, simulation_result)\n",
    "        \n",
    "#     def run(self):\n",
    "#         \"\"\"Run a single iteration, now reflecting the deeper default policy.\"\"\"\n",
    "#         self.forward()\n",
    "\n",
    "#     def choose_best_action(self):\n",
    "#         \"\"\"After running the MCTS, choose the best action based on the highest value of the final state.\"\"\"\n",
    "#         best_child = max(self.tree.children(self.tree.root), key=lambda n: self.V[n.state], default=None)\n",
    "#         if best_child:\n",
    "#             return best_child.action, best_child.state\n",
    "#         else:\n",
    "#             return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "#     env2 = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "#     # Load and compute the V-table from the Q-table\n",
    "#     q_table_file = 'Q.npy'  # Replace with the correct path to the Q-table file\n",
    "#     v_table = compute_v_table_from_q_table(q_table_file)\n",
    "    \n",
    "#     print(\"V-table:\", v_table)\n",
    "    \n",
    "#     # Define the target state\n",
    "#     target_state = 15\n",
    "    \n",
    "#     # Initialize starting state\n",
    "#     stateMCTS = env.reset()[0]\n",
    "#     print(f\"Initial state: {stateMCTS}\")\n",
    "#     state2 = env2.reset()[0]\n",
    "    \n",
    "#     path = [stateMCTS]\n",
    "#     terminal_state_reached = False\n",
    "#     max_iterations_without_convergence = 10000\n",
    "#     iteration_count = 0\n",
    "\n",
    "#     while not terminal_state_reached and iteration_count < max_iterations_without_convergence:\n",
    "#         # Initialize Monte Carlo Tree Search with the current state\n",
    "#         monteCarloTreeSearch = MonteCarloTreeSearch(env=env, V=v_table, initial_state=stateMCTS)\n",
    "#         monteCarloTreeSearch.run()\n",
    "#         print(f\"\\nBuilt tree for the cell\\n\")\n",
    "#         monteCarloTreeSearch.tree.show()\n",
    "        \n",
    "\n",
    "#         # Get the best action from the current state\n",
    "#         action, next_state = monteCarloTreeSearch.choose_best_action()\n",
    "\n",
    "#         if action is None:\n",
    "#             print(\"No valid actions found, stopping.\")\n",
    "#             break\n",
    "\n",
    "#         print(f\"Chosen action: {action}, leads to state: {next_state}\")\n",
    "#         path.append(next_state)\n",
    "\n",
    "#         # Apply the action in env2\n",
    "#         state2, reward, done, _, _ = env2.step(action)\n",
    "#         print(f\"New state after action {action}: {state2}, reward: {reward}, done: {done}\")\n",
    "\n",
    "#         # Check if the new state is the terminal state (goal) or if it is a terminal state (falling into the lake)\n",
    "#         if state2 == target_state:\n",
    "#             print(\"Target state reached.\")\n",
    "#             terminal_state_reached = True\n",
    "#         elif done:\n",
    "#             print(\"Fell into the lake, retrying.\")\n",
    "#             stateMCTS = env.reset()[0] \n",
    "#             env2.reset()  # Reset the environment and get the new initial state\n",
    "#             path = [stateMCTS]  # Reset the path for new attempt\n",
    "#             iteration_count = 0  # Reset the iteration count for new attempt\n",
    "#         else:\n",
    "#             # Update stateMCTS to continue MCTS from the new state\n",
    "#             stateMCTS = state2\n",
    "#             iteration_count += 1\n",
    "\n",
    "#     # Clear path if target state is not reached\n",
    "#     if not terminal_state_reached:\n",
    "#         path = []\n",
    "\n",
    "#     print(\"\\nFinal path (states):\", path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
